{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "935d787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016a7ab",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4430c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(classifier, X, y, N = 10 , ax = None ):\n",
    "    '''Utility function to plot decision boundary and scatter plot of data'''\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid( np.linspace(x_min, x_max, N), np.linspace(y_min, y_max, N))\n",
    "    classes = len(np.unique(y))\n",
    "    \n",
    "    #Check what methods are available\n",
    "    if hasattr(classifier, \"predict\"):\n",
    "        zz = np.array( [classifier.predict(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "    elif hasattr(classifier, \"predict_probabilities\"):\n",
    "        zz = np.array( [classifier.predict_proba(np.array([xi,yi]).reshape(1,-1))[:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "    else :\n",
    "        zz = np.array( [classifier(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "            \n",
    "    # reshape result and plot\n",
    "    Z = zz.reshape(xx.shape)\n",
    "    \n",
    "    \n",
    "    #Get current axis and plot\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, classes-1, cmap='viridis', alpha=.2)\n",
    "    ax.contour(xx, yy, Z,  classes-1, cmap='viridis')\n",
    "    ax.scatter(X[:,0],X[:,1],c = classifier.predict(X), cmap = 'viridis')\n",
    "    ax.set_xlabel('$X_1$')\n",
    "    ax.set_ylabel('$X_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17773e",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d989532",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_samples_path = \"./GMM.in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e2987d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> np.ndarray:\n",
    "    data = []\n",
    "    with open(gmm_samples_path) as f:\n",
    "        for line in f:\n",
    "            [x, y] = line.split(\" \")\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            data.append([x, y])\n",
    "    \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d72a328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7fde84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, num_components: int = 4, iter_limit: int = 100, eps: float = 0.001):\n",
    "        self.num_components = num_components\n",
    "        self.iter_limit = iter_limit\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Inner parameters\n",
    "        self.means = None\n",
    "        self.covariances = None\n",
    "        self.log_likelihoods = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray) -> None:\n",
    "\n",
    "        n, d = X.shape  ## n = datapoints, d = features\n",
    "        k = self.num_components  ## K number of clusters\n",
    "\n",
    "        # randomly initialize the starting means\n",
    "        mu = X[np.random.choice(n, k, replace = False)]\n",
    "\n",
    "        # initialize a covariance matrix for each gaussian\n",
    "        Sigma = [np.eye(d)] * k\n",
    "\n",
    "        # initialize the probability for each gaussian pi\n",
    "        pi = np.array([1 / k] * k)\n",
    "\n",
    "        # initialize responsibility matrix: n points for each gaussian\n",
    "        W = np.zeros((n,k))\n",
    "\n",
    "        # initialize list of log-likelihoods\n",
    "        log_likelihoods = []\n",
    "\n",
    "        # lambda function for gaussian pdf\n",
    "        P = lambda m, s: multivariate_normal.pdf(X, mean = m, cov = s)\n",
    "            \n",
    "        while len(log_likelihoods) < self.iter_limit:\n",
    "\n",
    "            # E step\n",
    "\n",
    "            # nominator of responsibilities: j is the j-th gaussian\n",
    "            for j in range(k):\n",
    "                W[:, j] = pi[j] * P(mu[j], Sigma[j])\n",
    "\n",
    "            # log likelihood computation (same as nominator of responsibilities)    \n",
    "            l = np.sum(np.log(np.sum(W, axis = 1)))\n",
    "\n",
    "            # store log likelihood in list\n",
    "            log_likelihoods.append(l)\n",
    "\n",
    "            # compute W matrix by dividing by denominator (the sum along j) \n",
    "            W = (W.T / W.sum(axis = 1)).T\n",
    "\n",
    "            # sum of w^i entries along j (used for parameter updates)\n",
    "            # these are the soft weighted number of datapoints belonging to each gaussian\n",
    "            W_s = np.sum(W, axis = 0)\n",
    "\n",
    "\n",
    "            # M step\n",
    "\n",
    "            for j in range(k):\n",
    "\n",
    "                ## Update means\n",
    "                mu[j] = (1. / W_s[j]) * np.sum(W[:, j] * X.T, axis = 1).T \n",
    "\n",
    "                ## Update covariances\n",
    "                Sigma[j] = ((W[:,j] * ((X - mu[j]).T)) @ (X - mu[j])) / W_s[j]\n",
    "\n",
    "                ## Update probabilities of each gaussian\n",
    "                pi[j] = W_s[j] / n\n",
    "\n",
    "            # check for convergence\n",
    "            if len(log_likelihoods) < 2: continue\n",
    "            if np.abs(l - log_likelihoods[-2]) < self.eps: break\n",
    "\n",
    "        self.means = mu\n",
    "        self.covariances = Sigma\n",
    "        self.log_likelihoods = log_likelihoods\n",
    "        \n",
    "    def predict_probabilities(self, x0: np.ndarray) -> np.ndarray:\n",
    "        probs = np.array([ multivariate_normal.pdf(x0, mean = self.means[j], cov = self.covariances[j]) for j in range(self.num_components) ])\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, x0: np.ndarray) -> np.ndarray:\n",
    "        probs = np.array([ multivariate_normal.pdf(x0, mean = self.means[j], cov = self.covariances[j]) for j in range(self.num_components) ])\n",
    "        return np.argmax(probs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db2d3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM()\n",
    "gmm.fit(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "805bc7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.099163  , -0.06309399],\n",
       "       [ 4.93536128, -0.02779437],\n",
       "       [-2.85908732,  6.93598154],\n",
       "       [-2.10300759, -5.12546242]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0f16bab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.87489786, 0.10117956],\n",
       "        [0.10117956, 1.12586096]]),\n",
       " array([[1.82469838, 0.85857754],\n",
       "        [0.85857754, 1.8899746 ]]),\n",
       " array([[ 2.68001822, -2.43363689],\n",
       "        [-2.43363689,  4.83792193]]),\n",
       " array([[ 3.68868973, -1.10814283],\n",
       "        [-1.10814283,  4.05763977]])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0c5c7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the points \n",
    "y = gmm.predict(x0 = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(gmm, d, y, N = 10 , ax = None )\n",
    "plt.title('EM-Gausian Mixture Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
