{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c43a21be",
   "metadata": {},
   "source": [
    "### Simple agent POMDP \n",
    "#### Author: Andrei Gabriel Popescu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6435c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "from enum import IntEnum\n",
    "from pprint import pprint\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fe6baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(IntEnum):\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "\n",
    "class States(IntEnum):\n",
    "    A0 = 0\n",
    "    A1 = 1\n",
    "    A2 = 2\n",
    "    A3 = 3\n",
    "    A4 = 4\n",
    "\n",
    "\n",
    "class Obs(IntEnum):\n",
    "    O_2 = 0 # two walls\n",
    "    O_3 = 1 # three walls\n",
    "\n",
    "class HomeworkEnv(object):\n",
    "    def __init__(self, max_num_steps: int = 1, noise: float = 0.15):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_num_steps\n",
    "            maximum number of steps allowed in the env\n",
    "        noise\n",
    "            observation noise. This means that with 1 - noise\n",
    "            you hear the tiger behind the correct door.\n",
    "        \"\"\"\n",
    "        self.max_num_steps = max_num_steps\n",
    "        self.noise = noise\n",
    "        self.num_steps = None\n",
    "        self.__state = None\n",
    "        self.done = True\n",
    "\n",
    "        # define state mapping\n",
    "        self.__num_states = 5\n",
    "        self.__state_mapping = {\n",
    "            States.A0: \"Agent in pos 0\",\n",
    "            States.A1: \"Agent in pos 1\",\n",
    "            States.A2: \"Agent in pos 2\", # Goal state\n",
    "            States.A3: \"Agent in pos 3\",\n",
    "            States.A4: \"Agent in pos 4\",\n",
    "        }\n",
    "\n",
    "        # define action mapping\n",
    "        self.__num_actions = 2\n",
    "        self.__action_mapping = {\n",
    "            Actions.LEFT: \"Left\",\n",
    "            Actions.RIGHT: \"Right\",\n",
    "        }\n",
    "\n",
    "        # define observation mapping\n",
    "        self.__num_obs = 2\n",
    "        self.__obs_mapping = {\n",
    "            Obs.O_2: \"Two walls\",\n",
    "            Obs.O_3: \"Three walls, end!\",\n",
    "        }\n",
    "\n",
    "        # init transitions & observations probabilities\n",
    "        # and rewards\n",
    "        self.__init_transitions()\n",
    "        self.__init_observations()\n",
    "        self.__init_rewards()\n",
    "\n",
    "    def __init_transitions(self):\n",
    "        # define transition probability for listening action\n",
    "        #               Tiger: left      Tiger: right\n",
    "        # Tiger: left    1.0              0.0\n",
    "        # Tiger: right   0.0              1.0\n",
    "        # This means that the tiger doesn't change location\n",
    "        left_action = np.array([[\n",
    "            [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 1.0, 0.0]\n",
    "        ]])\n",
    "\n",
    "        # define transition probability for left action\n",
    "        #               Tiger: left      Tiger: right\n",
    "        # Tiger: left    0.5              0.5\n",
    "        # Tiger: right   0.5              0.5\n",
    "        # Resets the tiger location\n",
    "        right_action =  np.array([[\n",
    "            [0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "        ]])\n",
    "        # define transition probability for the right action\n",
    "        #               Tiger: left      Tiger: right\n",
    "        # Tiger: left    0.5              0.5\n",
    "        # Tiger: right   0.5              0.5\n",
    "        # Resets the tiger location\n",
    "        stay = np.array([[\n",
    "            [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "        ]])\n",
    "        \n",
    "        self.__T = np.concatenate([left_action, right_action], axis=0)\n",
    "\n",
    "    def __init_observations(self):\n",
    "        # define observation probability for the listen action\n",
    "        #                Obs: Tiger left    Obs: Tiger right\n",
    "        # Tiger: left    0.85               0.15\n",
    "        # Tiger: right   0.15               0.85\n",
    "        obs_left = np.array([[\n",
    "            [0.0, 1.0],\n",
    "            [1.0, 0.0],\n",
    "            [1.0, 0.0],\n",
    "            [1.0, 0.0],\n",
    "            [0.0, 1.0]\n",
    "        ]])\n",
    "\n",
    "        # define observation probability for the left action\n",
    "        #                Obs: Tiger left    Obs: Tiger right\n",
    "        # Tiger: left    0.50               0.50\n",
    "        # Tiger: right   0.50               0.50\n",
    "        # Any observation without listening is informative\n",
    "        obs_right = np.array([[\n",
    "            [0.0, 1.0],\n",
    "            [1.0, 0.0],\n",
    "            [1.0, 0.0],\n",
    "            [1.0, 0.0],\n",
    "            [0.0, 1.0]\n",
    "        ]])\n",
    "        \n",
    "        # define observation probability for the right action\n",
    "        #                Obs: Tiger left    Obs: Tiger right\n",
    "        # Tiger: left    0.50               0.50\n",
    "        # Tiger: right   0.50               0.50\n",
    "        # Any observation without listening is informative\n",
    "        obs_stay = np.array([[\n",
    "            [0.0, 1.0],\n",
    "            [1.0, 0.0],\n",
    "            [1.0, 0.0],\n",
    "            [1.0, 0.0],\n",
    "            [0.0, 1.0]\n",
    "        ]])\n",
    "\n",
    "        self.__O = np.concatenate([obs_left, obs_right], axis=0)\n",
    "\n",
    "    def __init_rewards(self):\n",
    "        # define rewards for listening\n",
    "        # Tiger: left      -1\n",
    "        # Tiger: right     -1\n",
    "        # You get a -1 reward for listening\n",
    "        R_left = np.array([[-1, -1, -1, 0, -1]])\n",
    "\n",
    "        # define rewards for left action\n",
    "        # Tiger: left      -100\n",
    "        # Tiger: right      +10\n",
    "        # If the tiger is left and you choose to go\n",
    "        # left, then you get -100.\n",
    "        # If the tiger is right and you choose to go\n",
    "        # left, you get +10.\n",
    "        R_right = np.array([[-1, 0, -1, -1, -1]])\n",
    "\n",
    "        # define rewards for right action\n",
    "        # Tiger: left       +10\n",
    "        # Tiger: right      -100\n",
    "        # If the tiger is left and you choose to go\n",
    "        # right, then you get +10\n",
    "        # If the tiger is right and you choose to go\n",
    "        # right, then you get -100.\n",
    "        R_stay = np.array([[-1, -1, 0, -1, -1]]) # just in goal state\n",
    "        \n",
    "        self.__R = np.concatenate([R_left, R_right], axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.num_steps = 0\n",
    "\n",
    "        # initialize the state random\n",
    "        # this puts the tiger behind the left and right\n",
    "        # door with equal probability\n",
    "        self.__state = np.random.choice([States.A0, States.A4])\n",
    "\n",
    "    def step(self, action: Actions) -> Tuple[int, float, bool, Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Performs an environment step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action\n",
    "            action to be applied\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple containing the next observation, the reward,\n",
    "        ending episode flag, other information.\n",
    "        \"\"\"\n",
    "        assert not self.done, \"The episode finished. Call reset()!\"\n",
    "        self.num_steps += 1\n",
    "        self.done = (self.num_steps == self.max_num_steps)\n",
    "\n",
    "        # get the next observation. this is stochastic\n",
    "        obs = np.random.choice(\n",
    "            a=[Obs.O_2, Obs.O_3],\n",
    "            p=self.O[action][self.__state]\n",
    "        )\n",
    "\n",
    "        # get the reward. this is deterministic\n",
    "        reward = self.R[action][self.__state]\n",
    "\n",
    "        # get the next transition\n",
    "        self.__state = np.random.choice(\n",
    "            a=[States.A0, States.A1, States.A2, States.A3, States.A4],\n",
    "            p=self.T[action][self.__state]\n",
    "        )\n",
    "\n",
    "        # construct info\n",
    "        info = {\"num_steps\": self.num_steps}\n",
    "        return obs, reward, self.done, info\n",
    "\n",
    "    @property\n",
    "    def state_mapping(self) -> Dict[States, str]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        State mapping (for display purpose)\n",
    "        \"\"\"\n",
    "        return self.__state_mapping\n",
    "\n",
    "    @property\n",
    "    def action_mapping(self) -> Dict[Actions, str]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        Action mapping (for display purposes)\n",
    "        \"\"\"\n",
    "        return self.__action_mapping\n",
    "\n",
    "    @property\n",
    "    def obs_mapping(self) -> Dict[Obs, str]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        Observation mapping (for display purposes)\n",
    "        \"\"\"\n",
    "        return self.__obs_mapping\n",
    "\n",
    "    @property\n",
    "    def T(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        Transition probability matrix.\n",
    "        Axes: (a, s, s')\n",
    "        \"\"\"\n",
    "        return self.__T\n",
    "\n",
    "    @property\n",
    "    def O(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        Observation probability matrix.\n",
    "        Axes: (a, s, o)\n",
    "        \"\"\"\n",
    "        return self.__O\n",
    "\n",
    "    @property\n",
    "    def R(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        Reward matrix:\n",
    "        Axes: (a, s)\n",
    "        \"\"\"\n",
    "        return self.__R\n",
    "\n",
    "    @property\n",
    "    def states(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        List containing the states\n",
    "        \"\"\"\n",
    "        return list(self.__state_mapping.keys())\n",
    "\n",
    "    @property\n",
    "    def actions(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        List containing the actions\n",
    "        \"\"\"\n",
    "        return list(self.__action_mapping.keys())\n",
    "\n",
    "    @property\n",
    "    def obs(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        List containing the observations\n",
    "        \"\"\"\n",
    "        return list(self.__obs_mapping.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787a78c",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ba96379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(policies: Dict, ncols: int = 5):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    policies\n",
    "        Full dictionary of policies\n",
    "    ncol\n",
    "        Number of plots per row(columns)\n",
    "    \"\"\"\n",
    "\n",
    "    num_steps = len(policies)\n",
    "    nrows = num_steps // ncols + (num_steps % ncols != 0)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(20, 10))\n",
    "    for i in policies:\n",
    "        # print(\"Policy index: \", i)\n",
    "\n",
    "        row, col = i // ncols, i % ncols\n",
    "        # extract x1, y1 components\n",
    "        x1 = [b[0] for b in policies[i][\"policy\"]]\n",
    "        y1 = policies[i][\"scores\"]\n",
    "\n",
    "        # print(\"Nr policies: \", len(policies))\n",
    "        g = ax[row][col] if len(policies) > ncols else ax[col]\n",
    "        # plot scores\n",
    "        g.stem(x1, y1, 'o--', basefmt=\"b\")\n",
    "        g.set_title(f\"Value Function Step {i}\")\n",
    "\n",
    "        for v in policies[i][\"V\"]:\n",
    "            col1 = np.linspace(1, 0, 1000).reshape(-1, 1)\n",
    "            col2 = 1 - col1\n",
    "            matrix = np.concatenate([col1, col2], axis=1)\n",
    "\n",
    "            # compute points then filter\n",
    "            y2 = np.dot(matrix, v)\n",
    "            idx = (np.min(y1) <= y2) & (y2 <= np.max(y1))\n",
    "            g.plot(col2[idx], y2[idx], )\n",
    "\n",
    "\n",
    "def get_closest_belief(policy: Dict[Tuple, Actions], b) -> Tuple:\n",
    "    keys = list(np.array(x) for x in policy.keys())\n",
    "    dist = [np.linalg.norm(b - x) for x in keys]\n",
    "    return keys[np.argmin(dist)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2360094",
   "metadata": {},
   "source": [
    "### The next part is taken from the POMDP Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2515e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief(b: np.ndarray, a: Actions, o: Obs, env: HomeworkEnv) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the next belief state from the current belief state, applied action\n",
    "    and received observation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        Current belief state\n",
    "    a\n",
    "        Applied action\n",
    "    o\n",
    "        Observation received\n",
    "    env\n",
    "        Tiger Environment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Next belief state.\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract transition probability matrix\n",
    "    # adn observation probability matrix\n",
    "    T, O = env.T, env.O\n",
    "    \n",
    "    # get states, actions & observations\n",
    "    states, actions, obs = env.states, env.actions, env.obs\n",
    "    \n",
    "    # compute the next belief state\n",
    "    b_prime = np.zeros_like(b)\n",
    "    \n",
    "    ###########################\n",
    "    # TODO 1 - Your code here #\n",
    "    ###########################\n",
    "    \n",
    "    for s_prime in states:\n",
    "        sum_over_states = 0\n",
    "        for s in states:\n",
    "            sum_over_states += T[a][s][s_prime] * b[s]\n",
    "        b_prime[s_prime] = O[a][s_prime][o] * sum_over_states\n",
    "\n",
    "\n",
    "    # normalize\n",
    "    if b_prime.sum() != 0:\n",
    "        b_prime /= b_prime.sum()\n",
    "        \n",
    "    return b_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d9f0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(b: np.ndarray, buff: List[np.ndarray], eps: float = 1e-8) -> bool:\n",
    "    \"\"\"\n",
    "    Checks whether the belief is already in the buffer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        belief to check if it already exists\n",
    "    buff\n",
    "        buffer of beliefs to check against\n",
    "    eps\n",
    "        distance threshold\n",
    "    \"\"\"\n",
    "    dist = np.array([np.linalg.norm(b - x) for x in buff])\n",
    "    return any(dist < eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26914b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_beliefs(b: np.ndarray, env:HomeworkEnv) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates a list of possible beliefs that can result\n",
    "    form the current belief passed as argument\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        current belief\n",
    "    env\n",
    "        environment\n",
    "    \"\"\"\n",
    "    # get the list of possible actions\n",
    "    acts, obs = env.actions, env.obs\n",
    "    buff = []\n",
    "    \n",
    "    ######################\n",
    "    # TODO 2: Your code here\n",
    "    \n",
    "    # step 1: go through all the actions\n",
    "    for a in acts:\n",
    "        # step 2: go through all the observations\n",
    "        for o in obs:\n",
    "            # update current belief according to the\n",
    "            # current action and observation using \n",
    "            # the update_belief function previously implemented\n",
    "            # b_prime = ...\n",
    "            b_prime = update_belief(b, a, o, env)\n",
    "            # add the new belief to the buffer only\n",
    "            # if it is not a duplicate\n",
    "            #   ...\n",
    "            if not check_duplicate(b_prime, buff):\n",
    "                buff.append(b_prime)\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e675305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_beliefs(b_init: np.ndarray, env: HomeworkEnv) -> List[List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Generate all the possible belief that can result\n",
    "    in the maximum steps allowed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b_init\n",
    "        initial belief (we're going to use the [0.5, 0.5] for this lab).\n",
    "    env\n",
    "        environment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List of lists of belief, meaning that for each step\n",
    "    we will have a list of belief.\n",
    "    E.g.\n",
    "    [\n",
    "        [b_init],            ---> initial belief (level 1)\n",
    "        [b00, b01, b02, ...] ---> those result from the initial belief (level 2)\n",
    "        [b10, b11, b12, ...] ---> those result form the beliefs from the second level (level 3)\n",
    "        ....\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract the maximum number of steps allowe\n",
    "    # by the environment\n",
    "    max_num_steps = env.max_num_steps\n",
    "    \n",
    "    # initialize storing buffer by adding the \n",
    "    # list containing the initial belief\n",
    "    buff = [[b_init]]\n",
    "    \n",
    "    # for  the maximum steps allowed\n",
    "    for step in range(1, max_num_steps):\n",
    "        # buffer for the next level of beliefs\n",
    "        next_buff = []\n",
    "        \n",
    "        # go through all beliefs from the previous level\n",
    "        # and generate new ones\n",
    "        for b in buff[step - 1]:\n",
    "            # generate all the belief that can result for\n",
    "            # belief b (apply get_next_belief previously implemented)\n",
    "            tmp_buff = get_next_beliefs(b, env)\n",
    "            \n",
    "            # we have to check if the new beliefs\n",
    "            # don't exist already in the next level buffer\n",
    "            # so we don't add duplicates\n",
    "            for b_prime in tmp_buff:\n",
    "                if not check_duplicate(b_prime, next_buff):\n",
    "                    next_buff.append(b_prime)\n",
    "            \n",
    "        # add the new level of beliefs\n",
    "        buff.append(next_buff)\n",
    "    \n",
    "    return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d3a835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma_a_star(a: Actions, env: HomeworkEnv) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    a\n",
    "        current action\n",
    "    env\n",
    "        environment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    alpha^(a, *) vector. This in an array\n",
    "    of dimension: # of states and can be extracted\n",
    "    directly from the rewards matrix\n",
    "    \"\"\"\n",
    "    return env.R[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9ea7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma_a_o(a: Actions, o: Obs, V_prime: List[np.array], env: HomeworkEnv, gamma: float=0.9):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    a\n",
    "        action\n",
    "    o\n",
    "        observation\n",
    "    V_prime\n",
    "        list of alpha vectors from the next step\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discounting factor\n",
    "    \"\"\"\n",
    "    # get transition, observation and reward matrix\n",
    "    T, O, R = env.T, env.O, env.R\n",
    "    \n",
    "    # get posible states, actions and observations\n",
    "    states, actions, obs = env.states, env.actions, env.obs\n",
    "    \n",
    "    # buffer of next gamma_ao vectors\n",
    "    gamma_a_o = []\n",
    "    \n",
    "    # go through all alpha_prime vectors from V_prime\n",
    "    for alpha_prime in V_prime:\n",
    "        # define the new alpha_a_o vector\n",
    "        alpha_a_o = np.zeros((len(states)))\n",
    "        \n",
    "        ########################\n",
    "        # TODO 3: Your code here\n",
    "        # go throguh all states (s)\n",
    "        for s in states:\n",
    "        \n",
    "            # go through all states (s_prime)\n",
    "            for s_prime in states:\n",
    "        \n",
    "                # perform update alpha_a_o[s] += ...\n",
    "                alpha_a_o[s] += T[a][s][s_prime] * O[a][s_prime][o] * alpha_prime[s_prime]\n",
    "        ########################\n",
    "        \n",
    "        # append the new alpha_a_o vector to the buffer\n",
    "        gamma_a_o.append(gamma * alpha_a_o)\n",
    "    \n",
    "    return gamma_a_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9d7cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma_a_b(b: np.ndarray, \n",
    "                  V_prime: List[np.ndarray], \n",
    "                  env: HomeworkEnv, \n",
    "                  gamma: float=0.9) -> Dict[Actions, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        belief state\n",
    "    V_prime\n",
    "        list of alpha vector from the next step\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discounting factor\n",
    "    \"\"\"\n",
    "    # get all posible actions and observations\n",
    "    A, O = env.actions, env.obs\n",
    "    \n",
    "    # define gamma_a_b buffer \n",
    "    gamma_a_b = {}\n",
    "    \n",
    "    # go through all actions\n",
    "    for a in A:\n",
    "        # get the gamma_a_star vectors form the previously implemented function\n",
    "        gamma_a_star = get_gamma_a_star(a, env)\n",
    "        \n",
    "        # define accumulator accumulator\n",
    "        sum_gamma_a_o = np.zeros_like(gamma_a_star, dtype=np.float)\n",
    "        \n",
    "        # go through all the observations\n",
    "        for o in O:\n",
    "            # get gamma_a_o from the previously implementd function\n",
    "            gamma_a_o = get_gamma_a_o(a, o, V_prime, env, gamma)\n",
    "            \n",
    "            # need to do a maximization\n",
    "            best_alpha = None\n",
    "            best_score = -np.inf\n",
    "            \n",
    "            # go through all alphas from gamma_a_o\n",
    "            for alpha in gamma_a_o:\n",
    "                # compute the score by dot product between\n",
    "                # alpha and current belief b\n",
    "                score = np.dot(alpha, b)\n",
    "                \n",
    "                # update the best score and alpha\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_alpha = alpha\n",
    "            \n",
    "            # add best alpha to the summation\n",
    "            # notice that if V_prime is empty (for the last step)\n",
    "            # we don't have any best_alpha\n",
    "            if best_alpha is not None:\n",
    "                sum_gamma_a_o += best_alpha\n",
    "        \n",
    "        # add the reward vector to the accumulator\n",
    "        sum_gamma_a_o += gamma_a_star\n",
    "        \n",
    "        # store mapping between action and accumulator\n",
    "        gamma_a_b[a] = sum_gamma_a_o\n",
    "\n",
    "    return gamma_a_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d63c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V(B, V_prime, env: HomeworkEnv, gamma: float=0.9) -> Tuple[List[np.ndarray], Dict, List[float]]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    B\n",
    "        List of beliefs (per leve)\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discount factor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple containing the  the new V list, best policy for the current level, \n",
    "    and the best scores\n",
    "    \"\"\"\n",
    "    # define policy dictionary\n",
    "    policy = {}\n",
    "    \n",
    "    # define V and score buffers\n",
    "    V, scores = [], []\n",
    "    \n",
    "    # go through all beliefs\n",
    "    ################################################\n",
    "    # TODO 5: Your code here\n",
    "    for b in B:\n",
    "        # get gamma_a_b dictionary form the previous implemented function\n",
    "        # gamma_a_b = ....\n",
    "        gamma_a_b = get_gamma_a_b(b, V_prime, env, gamma)\n",
    "        \n",
    "        # variables for maximization\n",
    "        best_a = None\n",
    "        best_score = -np.inf\n",
    "        best_gamma_a_b = None\n",
    "        \n",
    "        # go through all actions from gamma_a_b\n",
    "        for a in gamma_a_b:\n",
    "            # compute score by dot product between\n",
    "            # the gamma_a_b corresponding to a and the current belief\n",
    "            # score = ...\n",
    "            score = np.dot(gamma_a_b[a], b)\n",
    "            # update score if better and\n",
    "            # remeber the action and the alpha\n",
    "            #    ... update best score, best_a, best_gamma_a_b\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_gamma_a_b = gamma_a_b[a]\n",
    "                best_a = a\n",
    "        \n",
    "        # remeber the action to be applied\n",
    "        # for the current belief state \n",
    "        policy[tuple(b)] = best_a\n",
    "        ###############################################\n",
    "        \n",
    "        # add best gamma_a_b to the V\n",
    "        V.append(best_gamma_a_b)\n",
    "        \n",
    "        # also remebere the best score\n",
    "        scores.append(best_score)   \n",
    "    \n",
    "    return V, policy, scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cf3998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_based_value_backup(env: HomeworkEnv, \n",
    "                             gamma: float=0.9):\n",
    "    \"\"\"\n",
    "    Point-based value backup algorithm for POMDP\n",
    "    Link: http://www.cs.cmu.edu/~ggordon/jpineau-ggordon-thrun.ijcai03.pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discount factor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Best policies per level. Each element\n",
    "    corresponds to a level\n",
    "    \"\"\"\n",
    "    \n",
    "    # define initial belief states\n",
    "    b_init = np.array([0.5, 0, 0, 0, 0.5])\n",
    "    \n",
    "    # generate the list of all possible beliefs per level\n",
    "    B = generate_all_beliefs(b_init, env)\n",
    "    \n",
    "    # need to reverse the list cause we are starting\n",
    "    # from the last possible acton\n",
    "    B = reversed(B)\n",
    "    \n",
    "    # initail list of best gamma_a_b\n",
    "    V = []\n",
    "    \n",
    "    # buffer of policies and V vectors\n",
    "    policies = {}\n",
    "    \n",
    "    # for each level and each set of beliefs\n",
    "    for i, bs in enumerate(B):\n",
    "        # get the V's, policy and the best scores\n",
    "        V, policy, scores = get_V(bs, V, env)\n",
    "        \n",
    "        # store results\n",
    "        policies[env.max_num_steps - i - 1] = {\n",
    "            \"policy\": policy,\n",
    "            \"V\": V,\n",
    "            \"scores\": scores\n",
    "        }\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f63e609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Step 0 ======== \n",
      "{(0.5, 0.0, 0.0, 0.0, 0.5): <Actions.LEFT: 0>}\n",
      "\n",
      "\n",
      "========== Step 1 ======== \n",
      "{(0.0, 0.0, 0.0, 0.0, 1.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 0.0, 1.0, 0.0): <Actions.LEFT: 0>,\n",
      " (0.0, 1.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>,\n",
      " (1.0, 0.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>}\n",
      "\n",
      "\n",
      "========== Step 2 ======== \n",
      "{(0.0, 0.0, 0.0, 0.0, 0.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 0.0, 0.0, 1.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 0.0, 1.0, 0.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 1.0, 0.0, 0.0): <Actions.LEFT: 0>,\n",
      " (0.0, 1.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>,\n",
      " (1.0, 0.0, 0.0, 0.0, 0.0): <Actions.LEFT: 0>}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Popescu Andrei\\AppData\\Local\\Temp\\ipykernel_17788\\1403614607.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sum_gamma_a_o = np.zeros_like(gamma_a_star, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "# define environment\n",
    "env = HomeworkEnv(max_num_steps=3)\n",
    "\n",
    "# solve environment\n",
    "policies = point_based_value_backup(env=env, gamma=0.9)\n",
    "\n",
    "for step in range(env.max_num_steps):\n",
    "    print(\"========== Step %d ======== \" % (step, ))\n",
    "    pprint(policies[step]['policy'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee45765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'policy': {(0.5, 0.0, 0.0, 0.0, 0.5): <Actions.LEFT: 0>}, 'V': [array([-2.629, -2.629, -3.439, -2.439, -1.81 ])], 'scores': [-2.2195]}\n",
      "Episode 0, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Right', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Two walls', 'Three walls, end!', 'Three walls, end!']\n",
      "\n",
      "\n",
      "Episode 1, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Left', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Three walls, end!', 'Two walls', 'Three walls, end!']\n",
      "\n",
      "\n",
      "Episode 2, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Left', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Three walls, end!', 'Two walls', 'Three walls, end!']\n",
      "\n",
      "\n",
      "Episode 3, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Left', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Three walls, end!', 'Two walls', 'Three walls, end!']\n",
      "\n",
      "\n",
      "Episode 4, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Right', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Two walls', 'Three walls, end!', 'Three walls, end!']\n",
      "\n",
      "\n",
      "Episode 5, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Right', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Two walls', 'Three walls, end!', 'Three walls, end!']\n",
      "\n",
      "\n",
      "Episode 6, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Right', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Two walls', 'Three walls, end!', 'Three walls, end!']\n",
      "\n",
      "\n",
      "Episode 7, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Right', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Two walls', 'Three walls, end!', 'Three walls, end!']\n",
      "\n",
      "\n",
      "Episode 8, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Left', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Three walls, end!', 'Two walls', 'Three walls, end!']\n",
      "\n",
      "\n",
      "Episode 9, Score: -4.00\n",
      "\t* Actions: ['Left', 'Right', 'Right', 'Left']\n",
      "\t* Obs: ['Three walls, end!', 'Two walls', 'Three walls, end!', 'Three walls, end!']\n",
      "\n",
      "\n",
      "=================\n",
      "Avg score: -4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Popescu Andrei\\AppData\\Local\\Temp\\ipykernel_17788\\1403614607.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sum_gamma_a_o = np.zeros_like(gamma_a_star, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "# define environment\n",
    "# you can change the number of steps\n",
    "env =HomeworkEnv(max_num_steps=4, noise=0.15)\n",
    "\n",
    "# solve environment\n",
    "policies = point_based_value_backup(env=env, gamma=0.9)\n",
    "print(policies[0])\n",
    "\n",
    "# do a few experiments\n",
    "scores = []\n",
    "num_experiments = 10\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    \n",
    "    # initial belief state\n",
    "    b = np.array([0.5, 0, 0, 0, 0.5])\n",
    "    \n",
    "    # score variable & buffer actions\n",
    "    score = 0\n",
    "    acts, obs = [], []\n",
    "    \n",
    "    for step in count():\n",
    "        # interact with the environment\n",
    "        b = get_closest_belief(policies[step][\"policy\"], b)\n",
    "        a = policies[step][\"policy\"][tuple(b)]\n",
    "        o, r, done, _ = env.step(a)\n",
    "        \n",
    "        # update score, acts & obs\n",
    "        score += r\n",
    "        acts.append(a)\n",
    "        obs.append(o)\n",
    "        \n",
    "        # break if environment completed\n",
    "        if done:\n",
    "            acts = [env.action_mapping[a] for a in acts]\n",
    "            obs = [env.obs_mapping[o] for o in obs]\n",
    "            \n",
    "            print(\"Episode %d, Score: %.2f\" % (i, score))\n",
    "            print(\"\\t* Actions:\", acts)\n",
    "            print(\"\\t* Obs:\", obs)\n",
    "            print(\"\\n\")\n",
    "            break\n",
    "        \n",
    "        # update belief\n",
    "        b = update_belief(b=b, a=a, o=o, env=env)\n",
    "    \n",
    "    # save score\n",
    "    scores.append(score)\n",
    "    \n",
    "# report mean score\n",
    "print(\"=================\")\n",
    "print(\"Avg score: %.2f\" % (np.mean(scores), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dee1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be12787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
