{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from itertools import count\n",
    "from typing import Union, Tuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size: int = 10000):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        size\n",
    "            Maximum number of transitions store in the buffer.\n",
    "            If the buffer overflows, older states are dropped.\n",
    "        \"\"\"\n",
    "        self.size    = size\n",
    "        self.length  = 0\n",
    "        self.idx     = -1\n",
    "        \n",
    "        # define buffers\n",
    "        self.states        = None\n",
    "        self.states_next   = None\n",
    "        self.actions       = None\n",
    "        self.rewards       = None\n",
    "        self.done          = None\n",
    "        \n",
    "    def store(self, \n",
    "              s: Union[torch.Tensor, np.ndarray], \n",
    "              a: int, \n",
    "              r: float, \n",
    "              s_next: Union[torch.Tensor, np.ndarray],\n",
    "              done: bool):\n",
    "        \n",
    "        \"\"\"\n",
    "        Stores one sample of experience\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s\n",
    "            Tensor encoding the current state.\n",
    "        a\n",
    "            Current action.\n",
    "        r\n",
    "            Current reward.\n",
    "        s_next\n",
    "            Tensor encoding the next state.\n",
    "        done\n",
    "            Done signal.\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize buffers\n",
    "        if self.states is None:\n",
    "            self.states      = torch.zeros([self.size] + list(s.shape))   # shape is (self.size, 4)\n",
    "            self.states_next = torch.zeros_like(self.states)              # shape is (self.size, 4)\n",
    "            self.actions     = torch.zeros((self.size, ))                 # shape is (self.size, )\n",
    "            self.rewards     = torch.zeros((self.size, ))                 # shape is (self.size, )\n",
    "            self.done        = torch.zeros((self.size, ))                 # shape is (self.size, ) \n",
    "        \n",
    "        # TODO: store current (s, a, r, s_next, done) behavior sample in the corresponding tensor buffers\n",
    "        # Note 1: older instances are overwritten if the buffer overflows.\n",
    "        # Note 2: increment buffer length after each update, until it reaches the maximum allowed value: self.size\n",
    "        if type(s) != torch.Tensor:\n",
    "            s = torch.from_numpy(s).view(1, -1)\n",
    "        \n",
    "        if type(s_next) != torch.Tensor:\n",
    "            s_next = torch.from_numpy(s_next).view(1, -1)\n",
    "        \n",
    "        a = torch.from_numpy(np.array([a], dtype=np.float16))\n",
    "        r = torch.from_numpy(np.array([r], dtype=np.float16))\n",
    "        if done == True:\n",
    "            done = torch.from_numpy(np.array([1], dtype=np.float16))\n",
    "        else:\n",
    "            done = torch.from_numpy(np.array([0], dtype=np.float16))\n",
    "\n",
    "        self.idx += 1\n",
    "        torch.cat([self.states[:(self.idx % self.size)], s, self.states[(self.idx % self.size):]], 0)\n",
    "        torch.cat([self.states_next[:(self.idx % self.size)], s_next, self.states_next[(self.idx % self.size):]], 0)\n",
    "        torch.cat([self.actions[:(self.idx % self.size)], a, self.actions[(self.idx % self.size):]], 0)\n",
    "        torch.cat([self.rewards[:(self.idx % self.size)], r, self.rewards[(self.idx % self.size):]], 0)\n",
    "        torch.cat([self.done[:(self.idx % self.size)], done, self.done[(self.idx % self.size):]], 0)\n",
    "            \n",
    "    def sample(self, batch_size: int = 128) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Sample a batch of experience\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size\n",
    "            Number of experience to sample\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of tensor consisting of a batch of states, actions, rewards, next states, done\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.length >= batch_size, \"Can not sample from the buffer yet\"\n",
    "        indices = np.random.choice(a=np.arange(self.length), size=batch_size, replace=False)\n",
    "        \n",
    "        # Sample (s, a, r, s_next, done) behavior samples   \n",
    "        s      = self.states[indices]\n",
    "        s_next = self.states_next[indices]\n",
    "        a = self.actions[indices]\n",
    "        r = self.rewards[indices]\n",
    "        done = self.done[indices]\n",
    "        \n",
    "        return s, a, r, s_next, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network achitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_RAM(nn.Module):\n",
    "    def __init__(self, in_features: int, num_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network for testing algorithm\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features\n",
    "            number of features of input.\n",
    "        num_actions\n",
    "            number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN_RAM, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # define architecture\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_generator(max_eps: float=1.0, min_eps: float=0.1, max_iter: int = 10000):\n",
    "    crt_iter = -1\n",
    "    \n",
    "    while True:\n",
    "        crt_iter += 1\n",
    "        frac = min(crt_iter/max_iter, 1)\n",
    "        eps = (1 - frac) * max_eps + frac * min_eps\n",
    "        yield eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epilson_greedy_action(Q: nn.Module, s: Tensor, eps: float):\n",
    "    rand = np.random.rand()\n",
    "    \n",
    "    # with prob eps select a random action\n",
    "    if rand < eps:\n",
    "        return np.random.choice(np.arange(Q.num_actions))\n",
    "    \n",
    "    # select best action\n",
    "    with torch.no_grad():\n",
    "        output = Q(s).argmax(dim=1).item()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: DQN target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def dqn_target(\n",
    "    Q: nn.Module,\n",
    "    target_Q: nn.Module,\n",
    "    r_batch: Tensor,\n",
    "    s_next_batch: Tensor,\n",
    "    done_batch: Tensor,\n",
    "    gamma: float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes DQN target\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q\n",
    "        Behavior Q network.\n",
    "    target_Q\n",
    "        Target Q network.\n",
    "    r_batch\n",
    "        Batch of rewards.\n",
    "    s_next_bacth\n",
    "        Batch of next states.\n",
    "    done_batch\n",
    "        Batch of done flag (1 means the episoded finished).\n",
    "    gamma\n",
    "        Discount factor.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Batch of DQN targets\n",
    "    \"\"\"\n",
    "    # cmpute next Q value based on which action gives max Q values\n",
    "    # Note:  decorator torch.no_grad() ensures that gradients computed based on next Q are not propagated to the target_Q network\n",
    "    # Note: take note of the done_batch values - if behavior sample i in the batch has a done flag (done_batch[i] = 1), then the next_Q_values[i] \n",
    "    #             will only consider the reward[i] (because there is no next_state)\n",
    "    \n",
    "    next_Q_values = target_Q(s_next_batch).max(dim=1)[0]\n",
    "    next_Q_values[done_batch == 1] = 0\n",
    "    return r_batch + (gamma * next_Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: DDQN target\n",
    "\n",
    "$target_t = R_{t+1} + \\gamma Q_{target}(S_{t+1}, \\underset{a}{\\operatorname{argmax}} Q(S_{t+1}, a; \\theta_{t}); \\theta_t^{-})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ddqn_target(\n",
    "    Q: nn.Module,\n",
    "    target_Q: nn.Module,\n",
    "    r_batch: Tensor,\n",
    "    s_next_batch: Tensor,\n",
    "    done_batch: Tensor,\n",
    "    gamma: float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes DQN target\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q\n",
    "        Behavior Q network.\n",
    "    target_Q\n",
    "        Target Q network.\n",
    "    r_batch\n",
    "        Batch of rewards.\n",
    "    s_next_bacth\n",
    "        Batch of next states.\n",
    "    done_batch\n",
    "        Batcho of done flag (1 means the episoded finished).\n",
    "    gamma\n",
    "        Discount factor.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Batch of DQN targets\n",
    "    \"\"\"\n",
    "    # cmpute next Q value based on which action gives max Q values\n",
    "    next_Q_values = target_Q(s_next_batch).gather(1, Q(s_next_batch).argmax(dim=1, keepdim=True)).squeeze()\n",
    "    next_Q_values[done_batch == 1] = 0\n",
    "    \n",
    "    # Compute the target of the current Q values\n",
    "    return r_batch + (gamma * next_Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Learning Alogrithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(\n",
    "    env: gym.Env,\n",
    "    targert_function: Callable,\n",
    "    batch_size: int = 128,\n",
    "    gamma: float = 0.99,\n",
    "    replay_buffer_size=10000,\n",
    "    num_episodes: int = 100000,\n",
    "    learning_starts: int = 1000,\n",
    "    learning_freq: int = 4,\n",
    "    target_update_freq: int = 100,\n",
    "    log_every: int = 100):\n",
    "\n",
    "    \"\"\"\n",
    "    DQN Learning\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env\n",
    "        gym environment to train on.\n",
    "    target_function\n",
    "        Function that computes the Q network target. For DQN - dqn_target, for DDQN - ddqn_target.\n",
    "    batch_size:\n",
    "        How many transitions to sample each time experience is replayed.\n",
    "    gamma\n",
    "        Discount Factor\n",
    "    replay_buffer_size\n",
    "        Replay buffer size.\n",
    "    num_episodes\n",
    "        number of episodes to run\n",
    "    learning_starts: int\n",
    "        After how many environment steps to start replaying experiences\n",
    "    learning_freq: int\n",
    "        How many steps of environment to take between every experience replay\n",
    "    target_update_freq: int\n",
    "        How many experience replay rounds (not steps!) to perform between\n",
    "        each update to the target Q network\n",
    "    log_every:\n",
    "        Logging interval\n",
    "    \"\"\"\n",
    "    # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "    input_arg = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # define device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize target q function and q function\n",
    "    Q = DQN_RAM(input_arg, num_actions).to(device)\n",
    "    target_Q = DQN_RAM(input_arg, num_actions).to(device)\n",
    "      \n",
    "    # Construct Q network optimizer function\n",
    "    optimizer = optim.Adam(Q.parameters(), lr=1e-3)\n",
    "    \n",
    "    # define criterion\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Construct the replay buffer\n",
    "    replay_buffer = ReplayBuffer()\n",
    "    \n",
    "    # define epsilon scheduler\n",
    "    eps_scheduler = iter(eps_generator())\n",
    "    \n",
    "    # define statistics buffer, total number of steps and total number of updates performed\n",
    "    all_episode_rewards = []\n",
    "    total_steps = 0\n",
    "    num_param_updates = 0\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # reset environment\n",
    "        s = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for _ in count():\n",
    "            # increse total number of steps\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Choose random action if not yet start learning\n",
    "            if total_steps > learning_starts:\n",
    "                eps = next(eps_scheduler)\n",
    "                s = torch.tensor(s).view(1, -1).float().to(device)\n",
    "                a = select_epilson_greedy_action(Q, s, eps)\n",
    "            else:\n",
    "                a = np.random.choice(np.arange(num_actions))\n",
    "\n",
    "            # advance one step\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            \n",
    "            # update episode rewards\n",
    "            episode_reward += r\n",
    "\n",
    "            # store other info in replay memory\n",
    "            replay_buffer.store(s, a, r, s_next, done)\n",
    "\n",
    "            # Resets the environment when reaching an episode boundary.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # update state\n",
    "            s = s_next\n",
    "\n",
    "            # perform experience replay and train the network.\n",
    "            if (total_steps > learning_starts and total_steps % learning_freq == 0):\n",
    "                for _ in range(learning_freq):\n",
    "                    if replay_buffer.length >= batch_size:\n",
    "                        # sample experinence from the replay buffer\n",
    "                        s_batch, a_batch, r_batch, s_next_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "                        # send everything to device\n",
    "                        s_batch      = s_batch.float().to(device)\n",
    "                        a_batch      = a_batch.long().to(device)\n",
    "                        r_batch      = r_batch.float().to(device)\n",
    "                        s_next_batch = s_next_batch.float().to(device)\n",
    "                        done_batch   = done_batch.long().to(device)\n",
    "\n",
    "                        # comput the q values according to the states and actions\n",
    "                        Q_values = Q(s_batch).gather(1, a_batch.unsqueeze(1)).view(-1)\n",
    "\n",
    "                        # Compute the target of the current Q values\n",
    "                        target_Q_values = targert_function(Q, target_Q, r_batch, s_next_batch, done_batch, gamma)\n",
    "\n",
    "                        # compute loss\n",
    "                        loss = criterion(target_Q_values, Q_values)\n",
    "\n",
    "                        # Clear previous gradients before backward pass\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        # increase number of updates\n",
    "                        num_param_updates += 1\n",
    "\n",
    "                        # Periodically update the target network by Q network to target Q network\n",
    "                        if num_param_updates % target_update_freq == 0:\n",
    "                            target_Q.load_state_dict(Q.state_dict())\n",
    "\n",
    "        # append total reward culumated\n",
    "        all_episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # log average reward over the last 100 episodes\n",
    "        if episode % log_every == 0 and total_steps > learning_starts:\n",
    "            mean_episode_reward = np.mean(all_episode_rewards[-100:])\n",
    "            print(\"Episode: %d, Mean reward: %.2f, Eps: %.2f\" % (episode, mean_episode_reward, eps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Learning\n",
    "\n",
    "#### Task 4a: Modify learning procedure to implement original DQN (model and target networks are the same)\n",
    "#### Task 4b: Modify learning procedure to implement target network DQN\n",
    "\n",
    "**Note: Experiment with different values of:**\n",
    "  - learning_freq\n",
    "  - target_update_frequency\n",
    "  - epsilon decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Mean reward: 21.49, Eps: 0.90\n",
      "Episode: 200, Mean reward: 20.24, Eps: 0.71\n",
      "Episode: 300, Mean reward: 17.70, Eps: 0.56\n",
      "Episode: 400, Mean reward: 13.59, Eps: 0.43\n",
      "Episode: 500, Mean reward: 11.66, Eps: 0.33\n",
      "Episode: 600, Mean reward: 11.20, Eps: 0.23\n",
      "Episode: 700, Mean reward: 10.66, Eps: 0.13\n",
      "Episode: 800, Mean reward: 10.04, Eps: 0.10\n",
      "Episode: 900, Mean reward: 9.83, Eps: 0.10\n",
      "Episode: 1000, Mean reward: 9.92, Eps: 0.10\n"
     ]
    }
   ],
   "source": [
    "# initialize gym env\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# DQN learning\n",
    "learning(\n",
    "    env=env,                          # gym environmnet\n",
    "    targert_function=dqn_target,      # dqn target construction\n",
    "    batch_size=128,                   # q-network update batch size\n",
    "    gamma=0.5,                       # discount factor\n",
    "    replay_buffer_size=10000,         # size of the replay buffer\n",
    "    num_episodes=1000,                # number of episodes to run\n",
    "    learning_starts=1000,             # number of initial random actions (exploration)\n",
    "    learning_freq=4,                  # frequency of the update\n",
    "    target_update_freq=100,           # number of gradient steps after which the target network is updated\n",
    "    log_every=100                     # logging interval. returns the mean reward per episode.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Learning\n",
    "**Note: Experiment with different values of:**\n",
    "  - learning_freq\n",
    "  - target_update_frequency\n",
    "  - epsilon decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Mean reward: 21.31, Eps: 0.90\n",
      "Episode: 200, Mean reward: 19.26, Eps: 0.72\n",
      "Episode: 300, Mean reward: 16.77, Eps: 0.57\n",
      "Episode: 400, Mean reward: 14.75, Eps: 0.44\n",
      "Episode: 500, Mean reward: 12.16, Eps: 0.33\n",
      "Episode: 600, Mean reward: 11.26, Eps: 0.23\n",
      "Episode: 700, Mean reward: 10.67, Eps: 0.13\n",
      "Episode: 800, Mean reward: 9.84, Eps: 0.10\n",
      "Episode: 900, Mean reward: 10.03, Eps: 0.10\n",
      "Episode: 1000, Mean reward: 9.88, Eps: 0.10\n"
     ]
    }
   ],
   "source": [
    "# initialize gym env\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# DQN learning\n",
    "learning(\n",
    "    env=env,                          # gym environmnet\n",
    "    targert_function=ddqn_target,     # dqn target construction\n",
    "    batch_size=128,                   # q-network update batch size\n",
    "    gamma=0.99,                       # discount factor\n",
    "    replay_buffer_size=10000,         # size of the replay buffer\n",
    "    num_episodes=1000,                # number of episodes to run\n",
    "    learning_starts=1000,             # number of initial random actions  (exploration)\n",
    "    learning_freq=4,                  # frequency of the update\n",
    "    target_update_freq=100,           # number of gradient steps after which the target network is updated\n",
    "    log_every=100                     # logging interval. returns the mean reward per episode.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
